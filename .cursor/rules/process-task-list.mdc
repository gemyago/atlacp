
# Task List Processing Guide

**For Small Context LLM Models - Focused TDD Workflow**

## MANDATORY RULES

### üö´ NEVER DO THESE:
- Work on multiple sub-tasks simultaneously
- Skip writing tests first
  - Exception: you are adding new data structures
- Add tests for data structures. Data structures should not be tested directly, but rather through the functions that use them.
- Implement before having a failing test

### ‚úÖ ALWAYS DO THESE:
- Read ONLY the current sub-task before starting
- Write ONE failing test for current sub-task only
- Verify test fails with assertion error (not compilation error)
- Implement minimal code to make test pass
- Run test to verify it passes
- Update task list immediately after completion

## STRICT WORKFLOW

When user provides `@process-task-list.mdc X.Y`:

### Step 1: FOCUS
- Read ONLY sub-task X.Y from the task list
- Do NOT read other sub-tasks
- Understand what this specific sub-task requires

### Step 2: WRITE FAILING TEST
- Create test for sub-task X.Y ONLY
- Follow Given/When/Then structure
- Avoid static variables shared across tests
- Use random data when possible, use faker (github.com/go-faker/faker/v4)
- Don't pollute testing namespace - if helper functions are only used within one test, nest them inside that test function
- Compare entire structs when possible instead of individual fields (e.g `assert.Equal(t, expectedUser, actualUser)`)
- Use factory functions to create reusable random data
- Test must fail with assertion error, not compilation error

### Step 3: VERIFY FAILURE
- Run test: `go test -v ./<package path> --run <test name>`
- Confirm test fails for the RIGHT reason (assertion, not compilation)
- If compilation error, add minimal stub and re-run test

### Step 4: MINIMAL IMPLEMENTATION
- Write ONLY the code needed to make THIS test pass
- Leave TODO comments for future sub-tasks
- Don't implement features for other sub-tasks

### Step 5: VERIFY SUCCESS
- Run test: `go test -v ./<package path> --run <test name>`
- Confirm test passes

### Step 6: UPDATE & STOP
- **MANDATORY**: Mark sub-task X.Y as `[x]` complete in task list
- **MANDATORY**: Update "Relevant Files" section if files were modified
- Say: "Sub-task X.Y complete. Should I continue with X.Z?"
- **STOP** - wait for user approval

## MANDATORY TASK LIST UPDATES

### ‚ö†Ô∏è CRITICAL REQUIREMENT ‚ö†Ô∏è
**You MUST update the task list file after completing each sub-task. This is NOT optional.**

### How to Update:
1. **Change checkbox**: `- [ ] X.Y Description` ‚Üí `- [x] X.Y Description`
2. **Update Relevant Files section** if any files were created/modified
3. **Save the task list file** before stopping

### Example Update:
```markdown
## Tasks
- [x] 1.1 Add publisher to protocol ‚úì
- [ ] 1.2 Add documentation comment
```

### Completion Checklist:
Before saying "Sub-task X.Y complete", verify:
- [ ] Test written and passes
- [ ] Code implemented
- [ ] Task list updated with `[x]`
- [ ] Relevant Files section updated (if applicable)
- [ ] Ready to ask user for approval

### If You Forget to Update:
- User will ask you to update the task list
- Always update immediately when reminded
- Don't proceed to next sub-task until task list is updated

## ADVANCED: MICRO-STEPPING FOR COMPLEX SUB-TASKS

When a sub-task involves multiple scenarios (happy path + edge cases), use micro-stepping:

### When to Use Micro-Steps:
- Sub-task involves error handling
- Multiple input validation scenarios
- Different execution paths based on conditions
- Complex business logic with edge cases

### Micro-Step Workflow:
1. **Assess Complexity**: Read the sub-task and determine if it needs micro-stepping
2. **Create Internal Todo**: If complex, create your own micro-step todo list:
   ```
   Internal Todo for Sub-task X.Y:
   - [ ] X.Y.a Happy path: [description]
   - [ ] X.Y.b Edge case: [description]
   - [ ] X.Y.c Edge case: [description]
   ```
3. **Follow Your Todo**: Work through each micro-step using TDD:
   ```
   For each micro-step:
   a) Write failing test for THIS scenario only
   b) Run test - verify it fails correctly
   c) Write minimal code to make THIS test pass
   d) Run test - verify it passes
   e) Mark micro-step complete in your internal todo
   ```

### Example Micro-Step Flow:
```
User: @process-task-list-v2.mdc 3.4

AI: "Sub-task 3.4 is complex. Creating internal todo:
- [ ] 3.4.a Happy path: acceptConnectionRequest updates state to .connected
- [ ] 3.4.b Edge case: handle invalid macId gracefully
- [ ] 3.4.c Edge case: emit connectionEstablished after state update

Working on 3.4.a..."
[follows TDD cycle]
"3.4.a complete ‚úì"

"Working on 3.4.b..."
[follows TDD cycle]
"3.4.b complete ‚úì"

"Working on 3.4.c..."
[follows TDD cycle]
"3.4.c complete ‚úì"

"All micro-steps complete. Sub-task 3.4 complete. Should I continue with 3.5?"
```

### Micro-Step Rules:
- **LLM creates its own micro-step todo** when sub-task is complex
- **Don't modify the original task list** - keep micro-steps internal
- Happy path ALWAYS comes first in your internal todo
- Each micro-step gets its own test
- Only use for genuinely complex sub-tasks
- Mark the original sub-task complete only when all micro-steps are done

## CONTEXT MANAGEMENT

### For Small Context Models:
- Focus on current sub-task only
- Don't load entire codebase
- Read only files mentioned in current sub-task
- Keep imports minimal
- Reference existing patterns from similar files

### File Reading Strategy:
1. Read current sub-task from task list
2. Read only files directly mentioned in sub-task
3. Look for existing patterns in similar files
4. Don't read entire large files unless necessary

## ERROR HANDLING

### If Test Won't Compile:
1. Add minimal stub implementation
2. Run test again
3. Verify test now fails with assertion error
4. Proceed with implementation

### If Test Passes Immediately:
1. Test is too weak or implementation already exists
2. Make test more specific to current sub-task
3. Ensure test actually verifies the new behavior

### If Multiple Tests Fail:
1. Fix compilation errors first
2. Focus only on making current sub-task test pass
3. Don't fix unrelated test failures in this iteration

## TASK LIST MAINTENANCE

### Marking Complete:
- Change `[ ]` to `[x]` for completed sub-task
- Update "Relevant Files" section with any new/modified files
- Mark parent task complete only when ALL sub-tasks are `[x]`

## EXAMPLE INTERACTION

```
User: @process-task-list-v2.mdc 1.1

AI Response:
1. Reading sub-task 1.1: "Add logic to publish document"
2. Writing failing test
3. Running test - fails as expected (e.g document repository not invoked)
4. Adding minimal implementation to publish document
5. Running test - passes
6. Updating task list: marking 1.1 as [x] complete

Sub-task 1.1 complete. Should I continue with 1.2?
```

## RECOVERY STRATEGIES

### If Stuck:
- Re-read current sub-task description
- Look for similar patterns in existing codebase
- Ask user for clarification on current sub-task only

### If Context Limit Reached:
- Focus on current sub-task only
- Don't read unnecessary files
- Use minimal imports and references

**Remember: One sub-task, one test, one implementation, one task list update, one confirmation. Keep it simple and focused.**
